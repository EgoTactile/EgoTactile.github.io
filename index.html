
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>EgoTactile</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <meta property="og:image" content="">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="682">
    <meta property="og:image:height" content="682">
    <meta property="og:type" content="website" />
    <meta property="og:url" content=""/>
    <meta property="og:title" content="EgoTactile" />
    <meta property="og:description" content="Project page for EgoTactile" />

        <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="EgoTactile" />
    <meta name="twitter:description" content="Project page for EgoTactile." />
    <meta name="twitter:image" content="" />


    <!--<link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
    <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>


    <style>
        .github-pill {
            border-radius: 50px;
            padding: 8px 20px;
            font-weight: 500;
            font-size: 16px;
            transition: background-color 0.3s ease;
            background-color: #000;
            color: #fff;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            text-decoration: none;
        }
        
        .github-pill:hover {
            background-color: #333;
            color: #fff;
            text-decoration: none;
        }
    </style>


</head>

<body>
    <br>
    <div class="container" id="main" style="width: 85%;">
    <!-- <div class="container" id="main"> -->
        <h2 class="col-md-8 col-md-offset-2 text-center" style="font-size: 28px; font-weight: bold; line-height: 1.5; color: #333;">
            <span style="color: #007BFF;">EgoTactile</span> : Learning Grasp Pressure for Everyday Objects from Egocentric Video
        </h2>
        </div>
        
        <!-- Make sure you include Font Awesome -->
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css"/>

        <div class="row">
            <div class="col-md-8 col-md-offset-2 text-center">
            <h3 style="font-family: 'Playfair Display', serif; font-style: italic; font-weight: 500; color: #333; font-size: 18px;">
                Anonymous Submission
            </h3>
            </div>
        </div>
        
        <div class="row">
            <div class="col-md-8 col-md-offset-2  text-center">
            <ul class="list-inline">
                <li>
                <a class="btn btn-dark github-pill" href="https://github.com/EgoTactile/EgoPressureDiff" target="_blank" role="button">
                    <i class="fab fa-github"></i> Code
                </a>
                </li>
                <li>
                <a class="btn btn-dark github-pill" href="https://huggingface.co/datasets/icml-2026-submission/EgoTactile" target="_blank" role="button">
                    <i class="fa-solid fa-database"></i> Hugging Face
                </a>
                </li>
            </ul>
            </div>
        </div>

        
        <div class="row">
            <!-- 左侧文本部分 -->
            <div class="col-md-4 col-md-offset-2">
                <h3 style="font-weight: bold;">
                    Abstract
                </h3>
                <p class="text-justify">
                    Estimating full-hand grasp pressure from egocentric video is critical for immersive VR and robotic manipulation, yet dense tactile sensing often relies on intrusive hardware. 
                    Existing vision-based methods predominantly rely on planar surfaces or fingertip contacts, failing to generalize to complex 3D object interactions. 
                    Therefore, we introduce EgoTactile, a benchmark pairing egocentric video with full-hand pressure supervision for diverse everyday objects, incorporating a bare-hand transfer subset to enable generalization to natural scenarios. 
                    Leveraging this benchmark, we first establish EgoPressureFormer as a discriminative baseline. Beyond this, to explicitly address the uncertainty in partial observations, we propose EgoPressureDiff, a conditional diffusion framework that adapts a large-scale pre-trained video diffusion backbone. By combining rich world knowledge priors with a Physically-Informed Feature Rectification layer to inject semantic constraints, our approach effectively hallucinates plausible contact patterns and resolves visual-physical ambiguities. 
                    Extensive experiments demonstrate that our method achieves superior performance on the benchmark and robust transferability to in-the-wild scenarios.
                </p>
            </div>
        
            <!-- 右侧图像部分 -->
            <div class="col-md-4 col-md-offset-0" style="margin-top: 80px;">
                <div style="width: 70%; margin: 0 auto;">
                    <img src="assets/task_overview.png" class="img-responsive" alt="teaser" style="width: 100%; margin: 0 auto;">
                </div>

                <p style="margin-top: 10px; font-size: 14px; color: #555; width: 100%; text-align: left;">
                    <strong>Figure 1:</strong> Task overview. Given an input RGB clip, the model predicts a pressure sequence or heatmap, optionally leveraging auxiliary Condition Info (e.g., masks and text metadata) to resolve physical ambiguities. The output is represented in two inter-convertible formats: a sparse sensor sequence and a dense spatial heatmap.
                </p>
            </div>
        </div>


        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
              <h3 style="font-weight: bold;">
                A. Task and Pressure Formulation
              </h3>
              <p class="text-justify">
                We study <em>egocentric grasp pressure prediction</em>, where the input is a fixed-length monocular RGB clip
                \(X=\{x_t\}_{t=1}^T\) sampled from an egocentric video stream, and the output is the per-frame pressure distribution during hand–object interaction. Optionally, the model can be conditioned on auxiliary signals
                \(C\), including per-frame segmentation masks \(m_t\) and clip-level text attributes that summarize object properties (e.g., weight, material) and subject attributes (e.g., age, gender, body fat), providing structured priors beyond raw pixels under occlusion and physical ambiguity.
                The target pressure at each time step is represented in two inter-convertible forms: (i) a sparse glove sensor vector \(p_t \in \mathbb{R}^M\) with \(M=162\) taxels, and (ii) a dense, hand-centric heatmap
                \(H_t \in \mathbb{R}^{S\times S}\) on a canonical canvas, with non-hand regions set to zero; predictions may be produced in either form and deterministically converted for unified evaluation.
                To address sensor noise and the large dynamic range, raw pressures are normalized to \([0,1]\) by zeroing values below \(p_{\min}=5\)N and scaling by a robust upper bound \(p_{\max}=200\)N (99.9th percentile), with clipping for outliers.{index=3}
                Conversion between modalities is implemented via a fixed linear rendering operator \(A\) (Gaussian diffusion on a canonical hand geometry) to project \(p_t\) into a heatmap, and an inverse recovery solved by ridge regression to map predicted heatmaps back to sensor space for evaluation.
                All methods are ultimately evaluated on aligned pressure sequences, reporting Temporal Accuracy for contact/non-contact dynamics, Contact IoU and Volumetric IoU for overlap of (thresholded and magnitude-aware) contact patterns, MAE for absolute pressure error, and a Part-wise Center-of-Pressure (CoP) Error to assess anatomical localization on fingertips and palm.
              </p>
            </div>
        </div> -->
        
        

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
              <h3 style="font-weight: bold;">
                A. EgoTactile Benchmark
              </h3>
              <p class="text-justify">
                EgoTactile benchmarks egocentric <em>full-hand</em> grasp pressure prediction by synchronizing wearable RGB video with high-resolution tactile measurements. The capture system records 1280×720 egocentric video at 30 fps (head- and neck-mounted viewpoints) together with a 162-sensor pressure glove (0–350 N, ~17 Hz), collected in a controlled green-screen setup with randomized lighting and object poses to improve robustness.
                To support both direct supervision and real-world transfer, the benchmark contains (i) a <strong>gloved-hand</strong> set where the pressure glove is visible and paired with synchronized pressure labels, and (ii) a <strong>bare-hand</strong> set where the visible hand is bare while an off-camera gloved hand performs a synchronized grasp to provide labels, guided by a metronome with varied tempo to avoid shortcut learning.
                The dataset spans 63 everyday objects across 7 categories and 12 participants (balanced gender), and includes anonymized object/subject metadata (e.g., weight/material/fill state; age/body weight/body fat/hand length) for controlled analysis and optional conditioning.
                Benchmark protocols evaluate both <strong>object generalization</strong> (Object-Held-Out) and <strong>cross-subject robustness</strong> (Subject-Held-Out) on the gloved-hand set, and Object-Held-Out transfer on the bare-hand set.
              </p>
          
              <br>
                <img width="95%" src="assets/dataset_overview.png" class="img-responsive" alt="pipeline" style="margin:auto">
                <p style="margin-top: 10px; font-size: 14px; color: #555; text-align: left;">
                    <strong>Figure 2:</strong>
                    Data collection setup and dataset statistics. Left: Our capture environment features controlled lighting and a green-screen background (a), and (b) illustrates the data collection scenario for bare-hand setting. To ensure viewpoint diversity and realistic transfer, we capture data using both head-mounted (c) and neck-mounted (d) cameras.   Right: Statistics of the collected data, including hand contact probability (e), average pressure heatmaps (g) and force magnitude distributions across objects (f) and participants (h).
                </p>
                <br>
            </div>
        </div>
                  



        <div class="row">
            <!-- Baselines -->
            <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <h3 style="font-weight: bold;">B. Baselines</h3>

                    <h4 style="font-weight: bold;">Baseline I: EgoPressureFormer</h4>
                    <p class="text-justify">
                        As a discriminative video-to-pressure sequence predictor, EgoPressureFormer builds on a TimeSformer backbone and avoids dense pixel-aligned heatmap regression under occlusion by using a query-based decoder: learnable sensor embeddings attend to spatiotemporal visual tokens to directly predict the full M-sensor pressure sequence. Training further mitigates contact sparsity via a multi-task objective combining per-sensor classification with a frame-level contact gate. However, as a deterministic regressor, it struggles to model the multi-modal distribution of pressure under severe occlusion.
                    </p>
                </div>
            </div>

            <div class="row">
                <!-- 左侧文本部分 -->
                <div class="col-md-4 col-md-offset-2">
                    <h4 style="font-weight: bold;">Baseline II: EgoPressureDiff</h4>
                    <p class="text-justify">
                        We cast egocentric pressure estimation as <em>conditional latent diffusion</em> to handle the inherent multi-modality caused by severe occlusion: multiple plausible pressure patterns can explain the same visible pixels, making deterministic regression ill-posed. EgoPressureDiff adapts a pre-trained Stable Video Diffusion (SVD) backbone to synthesize pressure heatmap sequences by denoising pressure latents conditioned on the RGB video, with both RGB clips and pressure heatmaps encoded into a shared latent space via SVD’s VAE and fused by channel concatenation in the denoiser.
                        As summarized in <strong>Figure 3 (EgoPressureDiff Training Pipeline)</strong>, the model further incorporates three complementary guidance signals to disambiguate contact: (i) <em>hint masks</em> as a structured spatial prior (processed by a lightweight Mask Encoder and injected into the latent input), (ii) <em>text prompts</em> encoding object/subject physical attributes, and (iii) a <em>pressure-heatmap prototype</em> providing an anatomical topology prior of where pressure can plausibly appear.
                    </p>
                </div>

                <!-- 右侧图像部分：图片 80% 居中；caption 用右侧列的全宽 -->
                <div class="col-md-4 col-md-offset-0" style="margin-top: 30px;">
                    <div style="width: 60%; margin: 0 auto;">
                        <img src="assets/egopressdiff_pipeline.png" class="img-responsive" alt="teaser" style="width: 100%; margin: 0 auto;">
                    </div>

                    <p style="margin-top: 10px; font-size: 14px; color: #555; width: 100%; text-align: left;">
                        <strong>Figure 3:</strong> EgoPressureDiff Training Pipeline. We formulate pressure estimation as a latent diffusion process conditioned on egocentric RGB video. To resolve physical ambiguities, we incorporate multi-modal guidance via: (i) a hint mask processed by a trainable Mask Encoder, and (ii) text prompts and a prototype heatmap injected through the proposed PIFR Layer.
                    </p>
                </div>
            </div>

            <div class="row">
                <!-- 左侧文本部分 -->
                <div class="col-md-4 col-md-offset-2">
                    <h4 style="font-weight: bold;">
                        Physically-Informed Feature Rectification (PIFR) layer
                    </h4>
                    <p class="text-justify">
                        A key challenge is that naive fusion of text (physically informative but spatially coarse) and prototypes (topologically stable but weak on magnitude) can yield unstable or physically implausible intensities, with the denoiser over-relying on the prototype. To address this, EgoPressureDiff introduces the <strong>Physically-Informed Feature Rectification (PIFR)</strong> module, which uses text-conditioned features to <em>rectify</em> the prototype-conditioned representation before it conditions SVD. Concretely (Figure 4), an intermediate denoiser feature map attends to prototype tokens and text tokens to form prototype-conditioned and text-conditioned features; a lightweight mapping network predicts affine modulation parameters <span style="font-style: italic;">&gamma;</span> and <span style="font-style: italic;">&beta;</span> from the text-conditioned feature, and applies them to calibrate the prototype-conditioned feature, preserving anatomical plausibility while injecting magnitude constraints implied by physical attributes. 
                    </p>
                </div>
            
                <!-- 右侧图像部分：图片 80% 居中；caption 用右侧列的全宽 -->
                <div class="col-md-4 col-md-offset-0" style="margin-top: 30px;">
                    <div style="width: 70%; margin: 0 auto;">
                        <img src="assets/PIFR_layer.png" class="img-responsive" alt="teaser" style="width: 100%; margin: 0 auto;">
                    </div>

                    <p style="margin-top: 10px; font-size: 14px; color: #555; width: 100%; text-align: left;">
                        <strong>Figure 4:</strong> (a) The original U-Net block of SVD. (b) Our proposed PIFR layer integrated into the U-Net block. Here, <span style="font-style: italic;">&gamma;</span> and <span style="font-style: italic;">&beta;</span> denote the scale and shift factor, respectively.
                    </p>
                </div>
            </div>

        </div>
          

        <div class="row"> 
            <div class="col-md-8 col-md-offset-2">
                <h3 style="font-weight: bold;">
                    C. Comparisons with Baselines
                </h3>
                <p class="text-justify">
                    Below we present qualitative hand-pressure prediction results produced by the baseline methods proposed in this paper on the same video clip. For PressureVision and EgoPressureFormer, the predicted outputs are discrete pressure classes; to visualize them as continuous heatmaps, we map each predicted class to its corresponding mean pressure value and render the resulting per-location pressure field as a heatmap.
                </p>
                <div class="row text-center">
                    <div style="display: flex; flex-direction: column; align-items: center; gap: 16px;">
                        <video width="90%" autoplay loop muted controls style="display:block; margin: 0 auto;">
                            <source src="assets/videos/Comparisions_videos/1_compat.mp4" type="video/mp4" />
                            Your browser does not support the video tag.
                        </video>
                        <video width="90%" autoplay loop muted controls style="display:block; margin: 0 auto;">
                            <source src="assets/videos/Comparisions_videos/2_compat.mp4" type="video/mp4" />
                            Your browser does not support the video tag.
                        </video>
                        <video width="90%" autoplay loop muted controls style="display:block; margin: 0 auto;">
                            <source src="assets/videos/Comparisions_videos/3_compat.mp4" type="video/mp4" />
                            Your browser does not support the video tag.
                        </video>
                        <video width="90%" autoplay loop muted controls style="display:block; margin: 0 auto;">
                            <source src="assets/videos/Comparisions_videos/4_compat.mp4" type="video/mp4" />
                            Your browser does not support the video tag.
                        </video>
                        <video width="90%" autoplay loop muted controls style="display:block; margin: 0 auto;">
                            <source src="assets/videos/Comparisions_videos/5_compat.mp4" type="video/mp4" />
                            Your browser does not support the video tag.
                        </video>
                    </div>
                </div>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3 style="font-weight: bold;"> 
                    D. Bare Hand Setting Results
                </h3>
                <p class="text-justify">
                The videos below showcase pressure generation by <strong>EgoPressureDiff</strong> in the bare-hand setting, demonstrating its ability to synthesize temporally coherent, anatomically plausible hand-pressure patterns conditioned on egocentric observations when the pressure glove is not visible.
                </p>

                <div style="overflow-x: auto; text-align: center;">
                    <table border="0" cellspacing="0" cellpadding="0" align="center" style="width: 100%;">
                        <tr>
                            <td align="center" valign="middle" width="50%">
                                <!-- Header row above the left video -->
                                <div style="display: flex; width: 99%; margin: 0 auto 6px auto;">
                                    <div style="flex: 1; text-align: center; font-weight: bold;">Ground Truth</div>
                                    <div style="flex: 1; text-align: center; font-weight: bold;">Input Video</div>
                                    <div style="flex: 1; text-align: center; font-weight: bold;">EgoPressureDiff</div>
                                </div>

                                <video id="v0" width="99%" autoplay loop muted controls>
                                    <source src="assets/videos/bare_hand_testset/1_compat.mp4" type="video/mp4" />
                                </video>
                            </td>
                            <td align="center" valign="middle" width="50%">
                                <!-- Header row above the left video -->
                                <div style="display: flex; width: 99%; margin: 0 auto 6px auto;">
                                    <div style="flex: 1; text-align: center; font-weight: bold;">Ground Truth</div>
                                    <div style="flex: 1; text-align: center; font-weight: bold;">Input Video</div>
                                    <div style="flex: 1; text-align: center; font-weight: bold;">EgoPressureDiff</div>
                                </div>

                                <video id="v1" width="99%" autoplay loop muted controls>
                                    <source src="assets/videos/bare_hand_testset/2_compat.mp4" type="video/mp4" />
                                </video>
                            </td>
                        </tr>
                    </table>
                </div>

                <div style="overflow-x: auto; text-align: center;">
                    <table border="0" cellspacing="0" cellpadding="0" align="center" style="width: 100%;">
                        <tr>
                            <td align="center" valign="middle" width="50%">
                                <video id="v2" width="99%" autoplay loop muted controls>
                                    <source src="assets/videos/bare_hand_testset/3_compat.mp4" type="video/mp4" />
                                </video>
                            </td>
                            <td align="center" valign="middle" width="50%">
                                <video id="v3" width="99%" autoplay loop muted controls>
                                    <source src="assets/videos/bare_hand_testset/4_compat.mp4" type="video/mp4" />
                                </video>
                            </td>
                        </tr>
                    </table>
                </div>

                <div style="overflow-x: auto; text-align: center;">
                    <table border="0" cellspacing="0" cellpadding="0" align="center" style="width: 100%;">
                        <tr>
                            <td align="center" valign="middle" width="50%">
                                <video id="v4" width="99%" autoplay loop muted controls>
                                    <source src="assets/videos/bare_hand_testset/7_compat.mp4" type="video/mp4" />
                                </video>
                            </td>
                            <td align="center" valign="middle" width="50%">
                                <video id="v5" width="99%" autoplay loop muted controls>
                                    <source src="assets/videos/bare_hand_testset/6_compat.mp4" type="video/mp4" />
                                </video>
                            </td>
                        </tr>
                    </table>
                </div>
            </div>
        </div>





        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3 style="font-weight: bold;"> 
                E. In The Wild Results
                </h3>
                <p class="text-justify">
                The videos below present pressure generation results from <strong>EgoPressureDiff</strong> in an in-the-wild, uncontrolled environment, illustrating its robustness to real-world variations in illumination, background clutter, and hand–object interaction dynamics while maintaining coherent, physically plausible pressure patterns.
                </p>

                <div style="overflow-x: auto; text-align: center;">
                    <table border="0" cellspacing="0" cellpadding="0" align="center" style="width: 100%;">
                        <tr>
                            <td align="center" valign="middle" width="33%">
                                <video id="v0" width="99%" autoplay loop muted controls>
                                    <source src="assets/videos/wild_testset/1_compat.mp4" type="video/mp4" />
                                </video>
                            </td>
                            <td align="center" valign="middle" width="33%">
                                <video id="v1" width="99%" autoplay loop muted controls>
                                    <source src="assets/videos/wild_testset/2_compat.mp4" type="video/mp4" />
                                </video>
                            </td>
                            <td align="center" valign="middle" width="33%">
                                <video id="v2" width="99%" autoplay loop muted controls>
                                    <source src="assets/videos/wild_testset/3_compat.mp4" type="video/mp4" />
                                </video>
                            </td>
                        </tr>
                    </table>
                </div>

                <div style="overflow-x: auto; text-align: center;">
                    <table border="0" cellspacing="0" cellpadding="0" align="center" style="width: 100%;">
                        <tr>
                            <td align="center" valign="middle" width="33%">
                                <video id="v3" width="99%" autoplay loop muted controls>
                                    <source src="assets/videos/wild_testset/4_compat.mp4" type="video/mp4" />
                                </video>
                            </td>
                            <td align="center" valign="middle" width="33%">
                                <video id="v4" width="99%" autoplay loop muted controls>
                                    <source src="assets/videos/wild_testset/5_compat.mp4" type="video/mp4" />
                                </video>
                            </td>
                            <td align="center" valign="middle" width="33%">
                                <video id="v5" width="99%" autoplay loop muted controls>
                                    <source src="assets/videos/wild_testset/6_compat.mp4" type="video/mp4" />
                                </video>
                            </td>
                        </tr>
                    </table>
                </div>

                <div style="overflow-x: auto; text-align: center;">
                    <table border="0" cellspacing="0" cellpadding="0" align="center" style="width: 100%;">
                        <tr>
                            <td align="center" valign="middle" width="33%">
                                <video id="v6" width="99%" autoplay loop muted controls>
                                    <source src="assets/videos/wild_testset/7_compat.mp4" type="video/mp4" />
                                </video>
                            </td>
                            <td align="center" valign="middle" width="33%">
                                <video id="v7" width="99%" autoplay loop muted controls>
                                    <source src="assets/videos/wild_testset/8_compat.mp4" type="video/mp4" />
                                </video>
                            </td>
                            <td align="center" valign="middle" width="33%">
                                <video id="v8" width="99%" autoplay loop muted controls>
                                    <source src="assets/videos/wild_testset/9_compat.mp4" type="video/mp4" />
                                </video>
                            </td>
                        </tr>
                    </table>
                </div>

                <div style="overflow-x: auto; text-align: center;">
                    <table border="0" cellspacing="0" cellpadding="0" align="center" style="width: 100%;">
                        <tr>
                            <td align="center" valign="middle" width="33%">
                                <video id="v9" width="99%" autoplay loop muted controls>
                                    <source src="assets/videos/wild_testset/10_compat.mp4" type="video/mp4" />
                                </video>
                            </td>
                            <td align="center" valign="middle" width="33%">
                                <video id="v10" width="99%" autoplay loop muted controls>
                                    <source src="assets/videos/wild_testset/11_compat.mp4" type="video/mp4" />
                                </video>
                            </td>
                            <td align="center" valign="middle" width="33%">
                                <video id="v11" width="99%" autoplay loop muted controls>
                                    <source src="assets/videos/wild_testset/12_compat.mp4" type="video/mp4" />
                                </video>
                            </td>
                        </tr>
                    </table>
                </div>

                <div style="overflow-x: auto; text-align: center;">
                    <table border="0" cellspacing="0" cellpadding="0" align="center" style="width: 100%;">
                        <tr>
                            <td align="center" valign="middle" width="33%">
                                <video id="v12" width="99%" autoplay loop muted controls>
                                    <source src="assets/videos/wild_testset/13_compat.mp4" type="video/mp4" />
                                </video>
                            </td>
                            <td align="center" valign="middle" width="33%">
                                <video id="v13" width="99%" autoplay loop muted controls>
                                    <source src="assets/videos/wild_testset/14_compat.mp4" type="video/mp4" />
                                </video>
                            </td>
                            <td align="center" valign="middle" width="33%">
                                <video id="v14" width="99%" autoplay loop muted controls>
                                    <source src="assets/videos/wild_testset/15_compat.mp4" type="video/mp4" />
                                </video>
                            </td>
                        </tr>
                    </table>
                </div>

            </div>
        </div>






        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3 style="font-weight: bold;"> 
                E. In The Wild Results on Unseen Objects
                </h3>
                <p class="text-justify">
                The video below shows <strong>EgoPressureDiff</strong> generating hand-pressure patterns for previously unseen objects in an uncontrolled, in-the-wild setting (an egg, a nutcracker figurine, and jasmine green tea). Notably, the model remains robust under this out-of-distribution object shift, producing temporally consistent and anatomically plausible pressure maps that qualitatively align with the expected grasp strategies induced by each object’s geometry and handling requirements.
                </p>

                <div style="overflow-x: auto; text-align: center;">
                    <table border="0" cellspacing="0" cellpadding="0" align="center" style="width: 100%;">
                        <tr>
                            <td align="center" valign="middle" width="33%">
                                <video id="v0" width="99%" autoplay loop muted controls>
                                    <source src="assets/videos/wild_testset-unseen/1_compat.mp4" type="video/mp4" />
                                </video>
                            </td>
                            <td align="center" valign="middle" width="33%">
                                <video id="v1" width="99%" autoplay loop muted controls>
                                    <source src="assets/videos/wild_testset-unseen/2_compat.mp4" type="video/mp4" />
                                </video>
                            </td>
                            <td align="center" valign="middle" width="33%">
                                <video id="v2" width="99%" autoplay loop muted controls>
                                    <source src="assets/videos/wild_testset-unseen/3_compat.mp4" type="video/mp4" />
                                </video>
                            </td>
                        </tr>
                    </table>
                </div>


            </div>
        </div>















        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                The website template was borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a> and <a href="https://jonbarron.info/mipnerf/">Mip-NeRF</a>.
                </p>
            </div>
        </div>

    </div>  
    <br><br><br>
</body>
</html>
